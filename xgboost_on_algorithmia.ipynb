{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\"\"\"\n",
    "When using a locally embedded Github action, this should point to the src path of the Github action itself. The suggested best practice is to place the Github action folder under a .github/actions directory under the root level of your model development repository\n",
    "An example usage would be:\n",
    "sys.path.append(\".github/actions/algorithmia_ci_modeldeployment/src\")\n",
    "\n",
    "If you're using a Github action on another repository: TODO\n",
    "\"\"\"\n",
    "sys.path.append(\"/src\")\n",
    "\n",
    "\n",
    "from action_config_utils import ActionConfigUtils\n",
    "\n",
    "config_utils = ActionConfigUtils(config_path=\".github/workflows/main.yml\")\n",
    "algo_name = config_utils.get_algoname(default_name=\"xgboost_automated\")\n",
    "modelfile_relativepath = config_utils.get_model_relativepath(default_path=\"./autodeployed_model.pkl\")\n",
    "print(algo_name)\n",
    "print(modelfile_relativepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with XGBoost on Algorithmia\n",
    "\n",
    "With this notebook, we will be training an XGBoost model on Amazon's Musical Instrument Reviews dataset and be able to use this model to predict the sentiment of the given texts. If you would like to see the final product first, you can check out this algorithm in action at https://algorithmia.com/algorithms/asli/xgboost_basic_sentiment_analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will step by step: \n",
    "\n",
    "1. Load the training data\n",
    "\n",
    "2. Preprocess the data\n",
    "\n",
    "3. Setup an XGBoost model and do a mini hyperparameter search\n",
    "\n",
    "4. Fit the data on our model\n",
    "\n",
    "5. Get the predictions\n",
    "\n",
    "6. Check the accuracy\n",
    "\n",
    "7. Pickle the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -r notebook_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "Let's load our training data, take a look at a few rows and one of the review texts in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/amazon_musical_reviews/Musical_instruments_reviews.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"reviewText\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Time to process our texts! Basically, we'll:\n",
    "- Remove the English stopwords\n",
    "- Remove punctuations\n",
    "- Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def threshold_ratings(data):\n",
    "    def threshold_overall_rating(rating):\n",
    "        return 0 if int(rating)<=3 else 1\n",
    "    data[\"overall\"] = data[\"overall\"].apply(threshold_overall_rating)\n",
    "\n",
    "def remove_stopwords_punctuation(data):\n",
    "    data[\"review\"] = data[\"reviewText\"] + data[\"summary\"]\n",
    "\n",
    "    puncs = list(punctuation)\n",
    "    stops = stopwords.words(\"english\")\n",
    "\n",
    "    def remove_stopwords_in_str(input_str):\n",
    "        filtered = [char for char in str(input_str).split() if char not in stops]\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    def remove_punc_in_str(input_str):\n",
    "        filtered = [char for char in input_str if char not in puncs]\n",
    "        return ''.join(filtered)\n",
    "\n",
    "    def remove_stopwords_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_stopwords_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    def remove_punc_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_punc_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    data[\"review\"] = remove_stopwords_in_series(data[\"review\"].str.lower())\n",
    "    data[\"review\"] = remove_punc_in_series(data[\"review\"].str.lower())\n",
    "\n",
    "def drop_unused_colums(data):\n",
    "    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', \"reviewText\", \"summary\"], axis=1, inplace=True)\n",
    "\n",
    "def preprocess_reviews(data):\n",
    "    remove_stopwords_punctuation(data)\n",
    "    threshold_ratings(data)\n",
    "    drop_unused_colums(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess_reviews(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "X = data[\"review\"]\n",
    "y = data[\"overall\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini randomized search\n",
    "Let's set up a very basic cross-validated randomized search over parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": range(9,12), \"min_child_weight\": range(5,8)}\n",
    "rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to vectorize, transform and fit\n",
    "Time to vectorize our data, transform it and then fit our model to it.\n",
    "To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model  = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', rand_search_cv)\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "Once we're happy with our model's accuracy, let's save it locally.\n",
    "\n",
    "Before we do that, #TODO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, modelfile_relativepath, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_script_path, algo_requirements_path = config_utils.get_algorithmia_filepaths(algo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $algo_script_path\n",
    "import Algorithmia\n",
    "import json\n",
    "import os.path\n",
    "import joblib\n",
    "import xgboost\n",
    "import pandas as pd\n",
    "\n",
    "#I'm generated via a notebook and pushed via Github Actions!\n",
    "\n",
    "client = Algorithmia.client()\n",
    "\n",
    "def load_model_config(config_rel_path=\"../model_config.json\"):\n",
    "    \"\"\"Loads the model manifest with the following structure\n",
    "    {\n",
    "      \"model_filepath\": uploaded model path on Algorithmia data collection\n",
    "      \"model_origin_repo\": model development repository, with the Github CI workflow\n",
    "      \"model_origin_commit_hash\": Commit SHA related to the trigger of the CI workflow\n",
    "      \"model_origin_commit_msg\": Commit message related to the trigger of the CI workflow\n",
    "      \"model_uploaded_utc\": UTC timestamp of the automated model upload\n",
    "    }\n",
    "    \"\"\"\n",
    "    config = []\n",
    "    config_path = \"{}/{}\".format(os.path.dirname(__file__), (config_rel_path))\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path) as json_file:\n",
    "            config = json.load(json_file)\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_model(config):\n",
    "    \"\"\"Loads the model object from the file at model_filepath key in config dict\"\"\"\n",
    "    model_path = config[\"model_filepath\"]\n",
    "    model_file = client.file(model_path).getFile().name\n",
    "    model_obj = joblib.load(model_file)\n",
    "    return model_obj\n",
    "\n",
    "\n",
    "def assert_model(model):\n",
    "    \"\"\"Just for demonstration purposes, asserts that the XGBoost model has the expected pipeline steps.\n",
    "    \"\"\"\n",
    "    assert xgb.steps[0][0] == \"vect\"\n",
    "    assert xgb.steps[1][0] == \"tfidf\"\n",
    "    assert xgb.steps[2][0] == \"model\"\n",
    "    print(\"All assertions are okay, we have a perfectly uploaded model!\")\n",
    "\n",
    "\n",
    "config = load_model_config()\n",
    "xgb = load_model(config)\n",
    "assert_model(xgb)\n",
    "\n",
    "# API calls will begin at the apply() method, with the request body passed as 'input'\n",
    "# For more details, see algorithmia.com/developers/algorithm-development/languages\n",
    "def apply(input):\n",
    "    series_input = pd.Series([input])\n",
    "    result = xgb.predict(series_input)\n",
    "    return {\n",
    "        \"sentiment\": result.tolist()[0], \n",
    "        \"predicting_model\": config[\"model_filepath\"],\n",
    "        \"model_origin\": \"Commit SHA: {} from repo: {}\".format(config[\"model_origin_commit_hash\"], config[\"model_origin_commit_msg\"], config[\"model_origin_repo\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $algo_requirements_path\n",
    "# I'm generated via a notebook and pushed via Github Actions!\n",
    "algorithmia>=1.0.0,<2.0\n",
    "scikit-learn\n",
    "pandas\n",
    "numpy\n",
    "joblib\n",
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}