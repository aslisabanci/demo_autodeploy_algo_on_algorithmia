{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with XGBoost on Algorithmia\n",
    "\n",
    "With this notebook, we will be training an XGBoost model on Amazon's Musical Instrument Reviews dataset and be able to use this model to predict the sentiment of the given texts. If you would like to see the final product first, you can check out this algorithm in action at https://algorithmia.com/algorithms/asli/xgboost_basic_sentiment_analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will step by step: \n",
    "\n",
    "1. Load the training data\n",
    "\n",
    "2. Preprocess the data\n",
    "\n",
    "3. Setup an XGBoost model and do a mini hyperparameter search\n",
    "\n",
    "4. Fit the data on our model\n",
    "\n",
    "5. Get the predictions\n",
    "\n",
    "6. Check the accuracy\n",
    "\n",
    "7. Pickle the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -r notebook_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "Let's load our training data, take a look at a few rows and one of the review texts in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       reviewerID        asin  \\\n0  A2IBPI20UZIR0U  1384719342   \n1  A14VAT5EAX3D9S  1384719342   \n2  A195EZSQDW3E21  1384719342   \n3  A2C00NNG1ZQQG2  1384719342   \n4   A94QU4C90B1AX  1384719342   \n\n                                       reviewerName   helpful  \\\n0  cassandra tu \"Yeah, well, that's just like, u...    [0, 0]   \n1                                              Jake  [13, 14]   \n2                     Rick Bennette \"Rick Bennette\"    [1, 1]   \n3                         RustyBill \"Sunday Rocker\"    [0, 0]   \n4                                     SEAN MASLANKA    [0, 0]   \n\n                                          reviewText  overall  \\\n0  Not much to write about here, but it does exac...      5.0   \n1  The product does exactly as it should and is q...      5.0   \n2  The primary job of this device is to block the...      5.0   \n3  Nice windscreen protects my MXL mic and preven...      5.0   \n4  This pop filter is great. It looks and perform...      5.0   \n\n                                 summary  unixReviewTime   reviewTime  \n0                                   good      1393545600  02 28, 2014  \n1                                   Jake      1363392000  03 16, 2013  \n2                   It Does The Job Well      1377648000  08 28, 2013  \n3          GOOD WINDSCREEN FOR THE MONEY      1392336000  02 14, 2014  \n4  No more pops when I record my vocals.      1392940800  02 21, 2014  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>reviewerName</th>\n      <th>helpful</th>\n      <th>reviewText</th>\n      <th>overall</th>\n      <th>summary</th>\n      <th>unixReviewTime</th>\n      <th>reviewTime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A2IBPI20UZIR0U</td>\n      <td>1384719342</td>\n      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n      <td>[0, 0]</td>\n      <td>Not much to write about here, but it does exac...</td>\n      <td>5.0</td>\n      <td>good</td>\n      <td>1393545600</td>\n      <td>02 28, 2014</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A14VAT5EAX3D9S</td>\n      <td>1384719342</td>\n      <td>Jake</td>\n      <td>[13, 14]</td>\n      <td>The product does exactly as it should and is q...</td>\n      <td>5.0</td>\n      <td>Jake</td>\n      <td>1363392000</td>\n      <td>03 16, 2013</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A195EZSQDW3E21</td>\n      <td>1384719342</td>\n      <td>Rick Bennette \"Rick Bennette\"</td>\n      <td>[1, 1]</td>\n      <td>The primary job of this device is to block the...</td>\n      <td>5.0</td>\n      <td>It Does The Job Well</td>\n      <td>1377648000</td>\n      <td>08 28, 2013</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A2C00NNG1ZQQG2</td>\n      <td>1384719342</td>\n      <td>RustyBill \"Sunday Rocker\"</td>\n      <td>[0, 0]</td>\n      <td>Nice windscreen protects my MXL mic and preven...</td>\n      <td>5.0</td>\n      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n      <td>1392336000</td>\n      <td>02 14, 2014</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A94QU4C90B1AX</td>\n      <td>1384719342</td>\n      <td>SEAN MASLANKA</td>\n      <td>[0, 0]</td>\n      <td>This pop filter is great. It looks and perform...</td>\n      <td>5.0</td>\n      <td>No more pops when I record my vocals.</td>\n      <td>1392940800</td>\n      <td>02 21, 2014</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/amazon_musical_reviews/Musical_instruments_reviews.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\""
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "data[\"reviewText\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Time to process our texts! Basically, we'll:\n",
    "- Remove the English stopwords\n",
    "- Remove punctuations\n",
    "- Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/aslisabanci/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def threshold_ratings(data):\n",
    "    def threshold_overall_rating(rating):\n",
    "        return 0 if int(rating)<=3 else 1\n",
    "    data[\"overall\"] = data[\"overall\"].apply(threshold_overall_rating)\n",
    "\n",
    "def remove_stopwords_punctuation(data):\n",
    "    data[\"review\"] = data[\"reviewText\"] + data[\"summary\"]\n",
    "\n",
    "    puncs = list(punctuation)\n",
    "    stops = stopwords.words(\"english\")\n",
    "\n",
    "    def remove_stopwords_in_str(input_str):\n",
    "        filtered = [char for char in str(input_str).split() if char not in stops]\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    def remove_punc_in_str(input_str):\n",
    "        filtered = [char for char in input_str if char not in puncs]\n",
    "        return ''.join(filtered)\n",
    "\n",
    "    def remove_stopwords_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_stopwords_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    def remove_punc_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_punc_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    data[\"review\"] = remove_stopwords_in_series(data[\"review\"].str.lower())\n",
    "    data[\"review\"] = remove_punc_in_series(data[\"review\"].str.lower())\n",
    "\n",
    "def drop_unused_colums(data):\n",
    "    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', \"reviewText\", \"summary\"], axis=1, inplace=True)\n",
    "\n",
    "def preprocess_reviews(data):\n",
    "    remove_stopwords_punctuation(data)\n",
    "    threshold_ratings(data)\n",
    "    drop_unused_colums(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   overall                                             review\n0        1  much write here exactly supposed to filters po...\n1        1  product exactly quite affordablei realized dou...\n2        1  primary job device block breath would otherwis...\n3        1  nice windscreen protects mxl mic prevents pops...\n4        1  pop filter great looks performs like studio fi...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overall</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>much write here exactly supposed to filters po...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>product exactly quite affordablei realized dou...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>primary job device block breath would otherwis...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>nice windscreen protects mxl mic prevents pops...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>pop filter great looks performs like studio fi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "preprocess_reviews(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "X = data[\"review\"]\n",
    "y = data[\"overall\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini randomized search\n",
    "Let's set up a very basic cross-validated randomized search over parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": range(9,12), \"min_child_weight\": range(5,8)}\n",
    "rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to vectorize, transform and fit\n",
    "Time to vectorize our data, transform it and then fit our model to it.\n",
    "To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n                ('model',\n                 RandomizedSearchCV(estimator=XGBClassifier(base_score=None,\n                                                            booster=None,\n                                                            colsample_bylevel=None,\n                                                            colsample_bynode=None,\n                                                            colsample_bytree=None,\n                                                            gamma=None,\n                                                            gpu_id=None,\n                                                            importance_type='gain',\n                                                            interaction_constraints=None,\n                                                            learning_rate=None,\n                                                            max_delta_step=None,\n                                                            max_depth=None,\n                                                            min_child_weight=None,\n                                                            missing=nan,\n                                                            monotone_constraints=None,\n                                                            n_estimators=100,\n                                                            n_jobs=None,\n                                                            num_parallel_tree=None,\n                                                            random_state=None,\n                                                            reg_alpha=None,\n                                                            reg_lambda=None,\n                                                            scale_pos_weight=None,\n                                                            subsample=None,\n                                                            tree_method=None,\n                                                            validate_parameters=None,\n                                                            verbosity=None),\n                                    n_iter=1,\n                                    param_distributions={'max_depth': range(9, 12),\n                                                         'min_child_weight': range(5, 8)}))])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model  = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', rand_search_cv)\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model Accuracy: 89.19\n"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "Once we're happy with our model's accuracy, let's save it locally first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "xgboost_automated\n./autodeployed_model.pkl\n"
    }
   ],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "workflow_yaml_path = \"/github/workspace/.github/workflows/main.yml\"\n",
    "#workflow_yaml_path = \"./.github/workflows/main.yml\"\n",
    "with open(workflow_yaml_path, 'r') as stream:\n",
    "    data_loaded = yaml.safe_load(stream)\n",
    "# pprint(data_loaded[\"jobs\"][\"algorithmia-ci\"])\n",
    "algo_name = data_loaded[\"jobs\"][\"algorithmia-ci\"][\"env\"][\"ALGORITHMIA_ALGONAME\"]\n",
    "modelfile_relativepath = data_loaded[\"jobs\"][\"algorithmia-ci\"][\"steps\"][2][\"with\"][\"modelfile_relativepath\"]\n",
    "print(algo_name)\n",
    "print(modelfile_relativepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['./autodeployed_model.pkl']"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "local_model_path = modelfile_relativepath\n",
    "joblib.dump(model, local_model_path, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_script_path = \"/github/workspace/{}/src/xgboost_automated.py\".format(algo_name)\n",
    "algo_req_path = \"/github/workspace/{}/requirements.txt\".format(algo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Writing /github/workspace/xgboost_automated/src/xgboost_automated.py\n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/github/workspace/xgboost_automated/src/xgboost_automated.py'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-14942d372d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'$algo_script_path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'import Algorithmia\\nimport json\\nimport os.path\\nimport joblib\\nimport xgboost\\nimport pandas as pd\\n\\n#I\\'m generated via a notebook and pushed via Github Actions!\\n\\nclient = Algorithmia.client()\\n\\ndef load_model_config(config_rel_path=\"../model_config.json\"):\\n    \"\"\"Loads the model manifest with the following structure\\n    {\\n      \"model_filepath\": uploaded model path on Algorithmia data collection\\n      \"model_origin_repo\": model development repository, with the Github CI workflow\\n      \"model_origin_commithash\": SHA of the commit, triggering the CI workflow\\n      \"model_uploaded_utc\": UTC timestamp of the automated model upload\\n    }\\n    \"\"\"\\n    config = []\\n    config_path = \"{}/{}\".format(os.path.dirname(__file__), (config_rel_path))\\n    if os.path.exists(config_path):\\n        with open(config_path) as json_file:\\n            config = json.load(json_file)\\n    return config\\n\\n\\ndef load_model(config):\\n    \"\"\"Loads the model object from the file at model_filePath key in config dict\"\"\"\\n    model_path = config[\"model_filePath\"]\\n    model_file = client.file(model_path).getFile().name\\n    loaded_xgb = joblib.load(model_file)\\n    return loaded_xgb\\n\\n\\nconfig = load_model_config()\\nxgb = load_model(config)\\nassert xgb.steps[0][0] == \"vect\"\\nassert xgb.steps[1][0] == \"tfidf\"\\nassert xgb.steps[2][0] == \"model\"\\nprint(\"All assertions are okay, we have a perfectly uploaded model!\")\\n\\n# API calls will begin at the apply() method, with the request body passed as \\'input\\'\\n# For more details, see algorithmia.com/developers/algorithm-development/languages\\ndef apply(input):\\n    series_input = pd.Series([input])\\n    result = xgb.predict(series_input)\\n    return {\\n        \"sentiment\": result.tolist()[0], \\n        \"predicting_model\": config[\"model_filepath\"],\\n        \"model_origin\": \"Commit SHA: {} from repo: {}\".format(config[\"model_origin_commithash\"], config[\"model_origin_repo\"])\\n    }\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-105>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/github/workspace/xgboost_automated/src/xgboost_automated.py'"
     ]
    }
   ],
   "source": [
    "%%writefile $algo_script_path\n",
    "import Algorithmia\n",
    "import json\n",
    "import os.path\n",
    "import joblib\n",
    "import xgboost\n",
    "import pandas as pd\n",
    "\n",
    "#I'm generated via a notebook and pushed via Github Actions!\n",
    "\n",
    "client = Algorithmia.client()\n",
    "\n",
    "def load_model_config(config_rel_path=\"../model_config.json\"):\n",
    "    \"\"\"Loads the model manifest with the following structure\n",
    "    {\n",
    "      \"model_filepath\": uploaded model path on Algorithmia data collection\n",
    "      \"model_origin_repo\": model development repository, with the Github CI workflow\n",
    "      \"model_origin_commithash\": SHA of the commit, triggering the CI workflow\n",
    "      \"model_uploaded_utc\": UTC timestamp of the automated model upload\n",
    "    }\n",
    "    \"\"\"\n",
    "    config = []\n",
    "    config_path = \"{}/{}\".format(os.path.dirname(__file__), (config_rel_path))\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path) as json_file:\n",
    "            config = json.load(json_file)\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_model(config):\n",
    "    \"\"\"Loads the model object from the file at model_filePath key in config dict\"\"\"\n",
    "    model_path = config[\"model_filePath\"]\n",
    "    model_file = client.file(model_path).getFile().name\n",
    "    loaded_xgb = joblib.load(model_file)\n",
    "    return loaded_xgb\n",
    "\n",
    "\n",
    "config = load_model_config()\n",
    "xgb = load_model(config)\n",
    "assert xgb.steps[0][0] == \"vect\"\n",
    "assert xgb.steps[1][0] == \"tfidf\"\n",
    "assert xgb.steps[2][0] == \"model\"\n",
    "print(\"All assertions are okay, we have a perfectly uploaded model!\")\n",
    "\n",
    "# API calls will begin at the apply() method, with the request body passed as 'input'\n",
    "# For more details, see algorithmia.com/developers/algorithm-development/languages\n",
    "def apply(input):\n",
    "    series_input = pd.Series([input])\n",
    "    result = xgb.predict(series_input)\n",
    "    return {\n",
    "        \"sentiment\": result.tolist()[0], \n",
    "        \"predicting_model\": config[\"model_filepath\"],\n",
    "        \"model_origin\": \"Commit SHA: {} from repo: {}\".format(config[\"model_origin_commithash\"], config[\"model_origin_repo\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Writing /github/workspace/xgboost_automated/requirements.txt\n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/github/workspace/xgboost_automated/requirements.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b0ff2ce51048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'$algo_req_path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# I'm generated via a notebook and pushed via Github Actions!\\nalgorithmia>=1.0.0,<2.0\\nscikit-learn\\npandas\\nnumpy\\njoblib\\nxgboost\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2372\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-105>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/github/workspace/xgboost_automated/requirements.txt'"
     ]
    }
   ],
   "source": [
    "%%writefile $algo_req_path\n",
    "# I'm generated via a notebook and pushed via Github Actions!\n",
    "algorithmia>=1.0.0,<2.0\n",
    "scikit-learn\n",
    "pandas\n",
    "numpy\n",
    "joblib\n",
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}